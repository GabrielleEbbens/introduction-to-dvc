---
title: "An Introduction to Data Version Control (DVC)"
author: "Gabrielle Ebbens"
date: "2026-02-12"
format:
  revealjs:
    center-title-slide: true

---
# {.title-slide}

## How is version control used currently? {style="text-align: center;"}

- File changes are tracked in remote git repositories
- Commits are snapshots of the project files at a given point in time
- Code files are easily stored in git repositories, like storing photos in a photo album
 
:::: {.columns}
 
::: {.column width="50%"}
![](An Introduction to Data Version Control (DVC)_files/images/Git Diagram.png){width=90%}
:::
 
::: {.column width="50%"}
![](An Introduction to Data Version Control (DVC)_files/images/photos into album.png){width=90%}
:::
 
::::

::: {.notes}
We all know what git is? This is the most common method of version control, where changes to files are tracked in a remote git repository in the form of commits (snapshots of the project files at a given point in time). Files are easily stored in git repositories, like storing photos in a photo album.
:::


## What makes version control difficult in data science?

- Models = data + code => infinite ways to change a model
- Lots of files => lots of changes(e.g. data_final.csv, final_data_2.csv,final_data_this_time_its_final.csv)
- HUGE DATASETS are too large to store in a git repository

![](An Introduction to Data Version Control (DVC)_files/images/yellow pages.png)

::: {.notes}
Models can be affected by a combination of data AND code, making version history more complex. There are infinite ways to change a dataset and models, meaning that there are A LOT of changes to keep track of. Many small changes to data may mean that we have lots of files named slightly differently (e.g. data.csv, data_2.csv, data_final.csv, data_final_the_finalest_version_of_final.csv). Data Scientists work with HUGE datasets, which are often too large to store in a git repository. It would be like trying to stick the yellow pages in a photo album.
:::

## Is there a better way to keep track of data and models? {style="text-align: center;"}

There is, Kevin!

![](An Introduction to Data Version Control (DVC)_files/images/joey.gif){width=300}


# Introducing DVC

:::: {.columns}

::: {.column width="40%"}

![](An Introduction to Data Version Control (DVC)_files/images/dvc.jpeg)
![](An Introduction to Data Version Control (DVC)_files/images/lake fs.png)
:::

::: {.column width="60%"}

- Open-source command-line tool owned by LakeFS
- Allows git-like versioning of data, models and pipelines
- Tracks the metadata of the data and models, while the actual data and models are stored in a remote storage (e.g. S3, Google Drive, Azure Blob Storage, locally).

:::

::::

## How can we use DVC?

```{mermaid}
%%| fig-width: 12
%%| fig-height: 7
%%| fig-align: center
flowchart LR
    A[Use DVC to track data] -->B(Initialise a dvc repository)
    B --> C(Add data to be tracked)
    C --> D(Link to remote storage)
    D --> E["All set up (yay!)"]
```

- DVC uses familiar git commands to track data and models:
  - `dvc init` to initialize a DVC project
  - `dvc add` to add data to be tracked
  - `dvc push` to push the data to the remote storage
  - `dvc pull` to pull the data from the remote storage
  - `dvc checkout` to checkout a previous version of the data

## How does our git know about the data?

- DVC uses a .dvc file to store the metadata of the data.
- The .dvc file is added to the git repository.
- The .dvc file contains the path to the data, the hash of the data, and the hash of the code that was used to generate the data.

(insert screenshot of .dvc file here)


## These are great commands! But what if I want to keep track of data at different stages of the machine learning lifecycle?
(insert gif or meme here)

## DVC pipelines

- The development of a machine learning model is often made of many stages (e.g. preprocessing, featurizing, training, evaluating).
- You could just run these commands sequentially, but who has the time for that?
- DVC allows set up these stages in a pipeline and keep track of the dependencies, the inputs and the outputs.
- You can then run this pipeline with a single command.

**Add a stage to the DVC pipeline**

```bash
dvc stage add -n preprocessing \
  -d src/preprocessing.py -d data/raw/lego_sets.csv \
  -o data/processed/preprocessed_lego_sets.csv \
  poetry run python src/preprocessing.py
```

- If you are running this command for the first time, it will add a dvc yaml file and a dvc lock file.
- Add any subsequent stages using `dvc stage add` or update the dvc.yaml file itself.

## dvc.yaml and dvc.lock files

- dvc.yaml: contains the pipeline definition
- dvc.lock: contains the pipeline state
(insert screenshot of dvc.yaml and dvc.lock here)

## Updating stages and reproducing the pipeline

- This is where dvc really shines.
- If one of the stages in the pipeline changes, dvc will only run that stage and the stages that depend on it.
- Running `dvc repro` will run the pipeline, and only the stages that need to be run will be run,
(screenshot of output).
- Each run of a pipeline is cached (in .dvc/cache/runs), and is utilized by `dvc repro` and `dvc exp run` (more on this later).

## DVC dag

- Sometimes it is easier to visualize the pipeline rather than reading the dvc.yaml file.
- DVC dag is a visualization of the pipeline.
- It shows the dependencies between the stages.
(screenshot of dvc dag)

## Parameters

- Parameters are values that can be changed to control the behavior of a stage.
  - This is helpful for hyperparameter tuning, for example.
- Parameters are specified in a params.yaml file (you will need to create this file).
- The code example shows how to add a stage and specify parameters.
(show screenshot of params file)

```bash
dvc stage add -n featurize \
  -p featurize.split_ratio -p featurize.random_seed \
  -d src/featurize.py -d data/processed/preprocessed_lego_sets.csv \
  -o data/featurized/features_train.csv \
  -o data/featurized/features_test.csv \
  -o data/featurized/targets_train.csv \
  -o data/featurized/targets_test.csv \
  poetry run python src/featurize.py
```

## Experiment tracking with DVC

- DVC allows you to track experiments and compare them.
- You can run `dvc exp run` to run an experiment.
- You can run `dvc exp list` to list all experiments.
- You can run `dvc exp diff` or `dvc metrics diff`to compare experiments.
(show screenshot of output)

## Experiment comparison with VSCode extension

- You can use the DVC extension for VSCode to compare experiments.
(show screenshot of VSCode extension experiment comparison)

## Queueing up experiments in DVC

- You can queue up experiments in DVC and have them run successively.
- This can be helpful for hyperparameter tuning.

```bash
dvc exp run --queue -S 'train.n_estimators=range(50, 100, 10)' -S 'train.max_depth=range(5, 10, 1)'
```
(screenshot of queued experiments)

- You can run `dvc queue start` to kick off the experiments.

## Output of experiment queue
(screenshot of experiment queue output)

## This is all well and good locally. But what about in the cloud?
(insert gif or meme here)

## Interaction with Cloud Services
 
 You can integrate DVC to work with other ML Frameworks, including:
 - Amazon SageMaker
 - Databricks
 - Evidently
 - Huggingface

 ## Running DVC from a remote git repository

 - DVC can be run in remote git environments (e.g. GitHub Actions or Bitbucket Pipelines).
 - You can setup access to your remote dvc repository in the dvc config file and run dvc commands as you would locally.
 - You can use this mechanism to automate your machine learning pipeline.
 (show screenshot of a GitHub Actions workflow or Bitbucket Pipeline)

 ## Conclusion

 - DVC allows you to keep track of data and models in a git-like method.
 - It is used in conjunction with git to keep track of data and models.
 - It allows you to orchestrate your machine learning pipeline and keep track of the dependencies, inputs, and outputs.
 - DVC's run cache allows you to avoid running stages that have not changed, improving speed and performance.
 - DVC can be used locally or in the cloud.
 - It is a powerful tool for anyone working with data and models.

 ## Any questions?

 ## Thanks for listening! 




