---
title: "An Introduction to Data Version Control (DVC)"
author: "Gabrielle Ebbens"
date: "2026-02-12"
format:
  revealjs:
    center-title-slide: true

---

## How is version control used currently? {style="text-align: center;"}

- File changes are tracked in remote git repositories
- Commits are snapshots of the project files at a given point in time
- Code files are easily stored in git repositories, like storing photos in a photo album
 
:::: {.columns}
 
::: {.column width="50%"}
![](An Introduction to Data Version Control (DVC)_files/images/Git Diagram.png){width=90%}
:::
 
::: {.column width="50%"}
![](An Introduction to Data Version Control (DVC)_files/images/photos into album.png){width=90%}
:::
 
::::

::: {.notes}
We all know what git is? This is the most common method of version control, where changes to files are tracked in a remote git repository in the form of commits (snapshots of the project files at a given point in time). Files are easily stored in git repositories, like storing photos in a photo album.
:::


## What makes version control difficult in data science?

- Models = data + code => infinite ways to change a model
- Lots of files => lots of changes(e.g. data_final.csv, final_data_2.csv,final_data_this_time_its_final.csv)
- HUGE DATASETS are too large to store in a git repository

![](An Introduction to Data Version Control (DVC)_files/images/yellow pages.png)

::: {.notes}
Models can be affected by a combination of data AND code, making version history more complex. There are infinite ways to change a dataset and models, meaning that there are A LOT of changes to keep track of. Many small changes to data may mean that we have lots of files named slightly differently (e.g. data.csv, data_2.csv, data_final.csv, data_final_the_finalest_version_of_final.csv). Data Scientists work with HUGE datasets, which are often too large to store in a git repository. It would be like trying to stick the yellow pages in a photo album.
:::

## Is there a better way to keep track of data and models? {style="text-align: center;"}

There is, Kevin!

![](An Introduction to Data Version Control (DVC)_files/images/joey.gif){width=300}


# Introducing DVC

:::: {.columns}

::: {.column width="30%"}

![](An Introduction to Data Version Control (DVC)_files/images/dvc.jpeg)
![](An Introduction to Data Version Control (DVC)_files/images/lake fs.png)
:::

::: {.column width="70%"}

- Open-source tool with a command-line interface and python API.
- Allows git-like versioning of data, models and pipelines
- Tracks the metadata of the data and models, while the actual data and models are stored in a remote storage (e.g. S3, Google Drive, Azure Blob Storage, locally).

:::

::::

## How can we use DVC?

```{mermaid}
flowchart LR
    A[Setting up a DVC project] --> |dvc init| B(Initialise a DVC repository)
    B --> |dvc add| C(Add data to be tracked) 
    C --> |dvc remote add| D(Link to remote storage)
```

![](An Introduction to Data Version Control (DVC)_files/images/dvc push and pull.png){width=90%}

::: {.notes}
How can we use dvc? A typical dvc setup workflow would look like this: first, we would initialise a dvc repository using `dvc init`, then we would add our data using `dvc add`, then we would link to our remote storage - which could be S3, Google Drive, Azure Blob Storage, or even locally - using `dvc remote add`. We would then push our data to the remote storage using `dvc push`, and pull our data from the remote storage using `dvc pull`, as we would do with git.

:::

## How does our git know about the data?

:::: {.columns}

::: {.column width="50%"}

![](An Introduction to Data Version Control (DVC)_files/images/dvc file.png)
:::

::: {.column width="50%"}

![](An Introduction to Data Version Control (DVC)_files/images/dvc file contents.png)
:::

::::

- DVC uses a .dvc file to store the metadata of the data. This is tracked in git.
- The .dvc file contains the path to the data, the hash of the data, and the hash of the code that was used to generate the data.

## That's great! But can we keep track of data at different stages of the machine learning lifecycle?

![](An Introduction to Data Version Control (DVC)_files/images/Schitts Creek Comedy GIF by CBC.gif)

## Yes you can with... DVC pipelines!

```{mermaid}
flowchart LR
    A[Setting up a DVC project] --> |dvc init| B(Initialise a DVC repository)
    B --> |dvc add| C(Add data to be tracked) 
    C --> |dvc remote add| D(Link to remote storage)
    D --> |dvc stage add| E(Set up a dvc pipeline or DAG)
```

![](An Introduction to Data Version Control (DVC)_files/images/Happy Parks And Rec GIF.gif)

## DVC pipelines

- With dvc, you can keep track of data and models at different stages of the machine learning lifecycle.

```bash
dvc stage add -n preprocessing \
  -d src/preprocessing.py -d data/raw/lego_sets.csv \
  -o data/processed/preprocessed_lego_sets.csv \
  poetry run python src/preprocessing.py
```
## dvc.yaml

:::: {.columns}

::: {.column width="50%"}

![](An Introduction to Data Version Control (DVC)_files/images/dvc yaml file.png){width=90%}

:::

::: {.column width="50%"}

- The dvc.yaml file contains the pipeline definition.
- Here, you can track each stage of the pipeline, including inputs,dependencies, outputs, and commands that run the stages.
- No need to run individual stages manually.

:::

::::

## dvc.lock file

:::: {.columns}

::: {.column width="50%"}

![](An Introduction to Data Version Control (DVC)_files/Images/dvc lock file.png)

:::

::: {.column width="50%"}

- The dvc.lock file contains the metadata of the pipeline.
- This is tracked in git along with the dvc.yaml file.

:::

::::

## Updating stages and reproducing the pipeline

- Run `dvc repro` to run the pipeline.
- If one of the stages in the pipeline changes, dvc will only run that stage and the stages that depend on it.
- This because each run of the pipeline is cached.

![](An Introduction to Data Version Control (DVC)_files/images/dvc repro.png){width=90%}

## DVC dag

:::: {.columns}

::: {.column witdth="50%"}

![](An Introduction to Data Version Control (DVC)_files/images/dvc dag.png){width=90%}

:::

::: {.column width="50%"}

- `dvc dag` shows a vizualization of the pipeline.

:::

::::

## Parameters

- Parameters are values that can be changed to control the behavior of a stage in the pipeline.

```bash
dvc stage add -n featurize \
  -p featurize.split_ratio -p featurize.random_seed \
  -d src/featurize.py -d data/processed/preprocessed_lego_sets.csv \
  -o data/featurized/features_train.csv \
  -o data/featurized/features_test.csv \
  -o data/featurized/targets_train.csv \
  -o data/featurized/targets_test.csv \
  poetry run python src/featurize.py
```
![](An Introduction to Data Version Control (DVC)_files/images/params.png){width=90%}

## Experiment tracking with DVC

- You can specify metrics to track in the dvc.yaml file.

![](An Introduction to Data Version Control (DVC)_files/images/exp run.png){width=90%}

![](An Introduction to Data Version Control (DVC)_files/images/metrics diff.png){width=90%}

## Experiment comparison with VSCode extension

- DVC has a VSCode extension to visualize experiments and their results.

![](An Introduction to Data Version Control (DVC)_files/images/vs code.png){width=90%}

## Queueing up experiments in DVC

- You can queue up experiments in DVC and have them run successively.
- This can be helpful for hyperparameter tuning.

```bash
dvc exp run --queue -S 'train.n_estimators=range(50, 100, 10)' -S 'train.max_depth=range(5, 10, 1)'
```
![](An Introduction to Data Version Control (DVC)_files/images/dvc queue.png){width=90%}

- You can run `dvc queue start` to kick off the experiments.

## This is all well and good locally. But what about in the cloud?
(insert gif or meme here)

## Interaction with Cloud Services
 
 - You can integrate DVC to work with other ML Frameworks, including:
  - Amazon SageMaker
  - Databricks
  - Evidently
  - Huggingface


## Running DVC from a remote git repository

 - DVC can be run in remote git environments (e.g. GitHub Actions or Bitbucket Pipelines).
 - You can setup access to your remote dvc repository in the dvc config file and run dvc commands as you would locally.
 - You can use this mechanism to automate your machine learning pipeline.


## Summary

 - DVC allows you to keep track of data and models in a git-like method.
 - It is used with git to keep track of data and models.
 - It allows you to orchestrate your machine learning pipeline and keep track of the dependencies, inputs, and outputs.
 - DVC's run cache allows you to avoid running stages that have not changed, improving speed and performance.
 - DVC can be used locally or in the cloud.

## Any questions?

![](An Introduction to Data Version Control (DVC)_files/images/Feelings Thoughts GIF by chuber channel.gif){width=90%}

## Social Links

- GitHub: https://github.com/GabrielleEbbens
- LinkedIn: https://www.linkedin.com/in/gabrielle-ebbens/




